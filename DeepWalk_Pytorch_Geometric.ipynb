{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBFmFiBQOpq9"
      },
      "source": [
        "# DEEPWALK IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rmvSUJHxkNvM"
      },
      "outputs": [],
      "source": [
        "PATH = '/Users/silviaarellanogarcia/Documents/MSc MACHINE LEARNING/Advanced Machine Learning/Project/Datasets/Flickr-dataset'\n",
        "SAVE_PATH = '/Users/silviaarellanogarcia/Documents/MSc MACHINE LEARNING/Advanced Machine Learning/Project/embeddings_deepwalk'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSnkZOaIn48c",
        "outputId": "302a0e26-37ee-4e96-a6d3-f84eedc06792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting umap-learn\n",
            "  Using cached umap_learn-0.5.5-py3-none-any.whl\n",
            "Collecting numpy>=1.17 (from umap-learn)\n",
            "  Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scipy>=1.3.1 (from umap-learn)\n",
            "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "Collecting scikit-learn>=0.22 (from umap-learn)\n",
            "  Using cached scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numba>=0.51.2 (from umap-learn)\n",
            "  Using cached numba-0.58.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Using cached pynndescent-0.5.11-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting tqdm (from umap-learn)\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba>=0.51.2->umap-learn)\n",
            "  Using cached llvmlite-0.41.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: importlib-metadata in ./.venv/lib/python3.8/site-packages (from numba>=0.51.2->umap-learn) (7.0.1)\n",
            "Collecting joblib>=0.11 (from pynndescent>=0.5->umap-learn)\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.22->umap-learn)\n",
            "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.8/site-packages (from importlib-metadata->numba>=0.51.2->umap-learn) (3.17.0)\n",
            "Using cached numba-0.58.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Using cached pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
            "Using cached scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Using cached llvmlite-0.41.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
            "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: tqdm, threadpoolctl, numpy, llvmlite, joblib, scipy, numba, scikit-learn, pynndescent, umap-learn\n",
            "Successfully installed joblib-1.3.2 llvmlite-0.41.1 numba-0.58.1 numpy-1.24.4 pynndescent-0.5.11 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.2.0 tqdm-4.66.1 umap-learn-0.5.5\n",
            "Collecting networkx\n",
            "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "Installing collected packages: networkx\n",
            "Successfully installed networkx-3.1\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in ./.venv/lib/python3.8/site-packages (from gensim) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in ./.venv/lib/python3.8/site-packages (from gensim) (1.10.1)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Using cached smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Using cached gensim-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "Using cached smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "Installing collected packages: smart-open, gensim\n",
            "Successfully installed gensim-4.3.2 smart-open-6.4.0\n",
            "Collecting seaborn\n",
            "  Using cached seaborn-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.7.4-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.venv/lib/python3.8/site-packages (from seaborn) (1.24.4)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Using cached contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Using cached fonttools-4.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (157 kB)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
            "  Using cached kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (23.2)\n",
            "Collecting pillow>=6.2.0 (from matplotlib)\n",
            "  Using cached Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
            "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
            "  Using cached importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas)\n",
            "  Using cached tzdata-2023.4-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Using cached seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
            "Using cached matplotlib-3.7.4-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
            "Using cached pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "Using cached contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "Using cached importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
            "Using cached kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "Using cached Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
            "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "Using cached tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
            "Installing collected packages: pytz, tzdata, pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, pandas, matplotlib, seaborn\n",
            "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.47.0 importlib-resources-6.1.1 kiwisolver-1.4.5 matplotlib-3.7.4 pandas-2.0.3 pillow-10.1.0 pyparsing-3.1.1 pytz-2023.3.post1 seaborn-0.13.0 tzdata-2023.4\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.8/site-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\n",
            "Collecting scikit-multilearn\n",
            "  Using cached scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "Installing collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m336.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in ./.venv/lib/python3.8/site-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from torch-geometric) (1.24.4)\n",
            "Requirement already satisfied: scipy in ./.venv/lib/python3.8/site-packages (from torch-geometric) (1.10.1)\n",
            "Collecting jinja2 (from torch-geometric)\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting requests (from torch-geometric)\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pyparsing in ./.venv/lib/python3.8/site-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.8/site-packages (from torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in ./.venv/lib/python3.8/site-packages (from torch-geometric) (5.9.7)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch-geometric)\n",
            "  Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->torch-geometric)\n",
            "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torch-geometric)\n",
            "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->torch-geometric)\n",
            "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torch-geometric)\n",
            "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
            "Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached idna-3.6-py3-none-any.whl (61 kB)\n",
            "Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "Installing collected packages: urllib3, MarkupSafe, idna, charset-normalizer, certifi, requests, jinja2, torch-geometric\n",
            "Successfully installed MarkupSafe-2.1.3 certifi-2023.11.17 charset-normalizer-3.3.2 idna-3.6 jinja2-3.1.2 requests-2.31.0 torch-geometric-2.4.0 urllib3-2.1.0\n",
            "Collecting torch\n",
            "  Downloading torch-2.1.2-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.8/site-packages (from torch) (4.9.0)\n",
            "Collecting sympy (from torch)\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in ./.venv/lib/python3.8/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from torch) (3.1.2)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0 (from torch)\n",
            "  Downloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\n",
            "Collecting mpmath>=0.19 (from sympy->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.1.2-cp38-cp38-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:03\u001b[0mm\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "Successfully installed filelock-3.13.1 fsspec-2023.12.2 mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.1.2 triton-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn\n",
        "!pip install networkx\n",
        "!pip install gensim\n",
        "!pip install seaborn matplotlib pandas\n",
        "!pip install scikit-learn\n",
        "!pip install scikit-multilearn\n",
        "!pip install torch-geometric\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fcBiplQ_OpQe"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "from gensim.models import Word2Vec # Needed for Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "from multiprocessing import cpu_count\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.datasets import CitationFull\n",
        "from torch_geometric.datasets import HeterophilousGraphDataset\n",
        "from torch_geometric.datasets import Reddit\n",
        "from torch_geometric.datasets import PPI\n",
        "\n",
        "from torch_geometric.utils import to_networkx\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import torch\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from six import string_types\n",
        "from gensim.models.word2vec import Vocab\n",
        "\n",
        "import scipy.io as sio\n",
        "from scipy import sparse\n",
        "\n",
        "from collections import defaultdict\n",
        "from scipy.sparse import coo_matrix\n",
        "from sklearn.utils import shuffle as skshuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OvFVE6fquPPh"
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "window_size = 10 ## 10 --> Suggested in the Khosla comparative paper\n",
        "embedding_size = 128\n",
        "walks_per_vertex = 80 ##80 --> Suggested in the Khosla comparative paper\n",
        "walk_length = 40 #40 --> Suggested in the Khosla comparative paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwbpZmS_hoAK"
      },
      "source": [
        "### IMPLEMENTATION OF THE DEEPWALK METHOD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2DCARpZ5Ulbi"
      },
      "outputs": [],
      "source": [
        "def Random_Walk(G, vi, t):\n",
        "  '''\n",
        "  Inputs:\n",
        "    G: Graph\n",
        "    vi: initial vertex of the random walk\n",
        "    t: walk length (the walks could have different length according to the paper)\n",
        "  Output:\n",
        "    Wvi: sequence of vertices visited in the random walk (starting from vi).\n",
        "  '''\n",
        "  Wvi = []\n",
        "  Wvi.append(str(vi)) # The initial vertex is always visited\n",
        "  last_visited = vi\n",
        "\n",
        "  for i in range(t):\n",
        "    neighbors_last_vi = list(G.neighbors(last_visited))\n",
        "    last_visited = random.choice(neighbors_last_vi)\n",
        "    Wvi.append(str(last_visited))\n",
        "\n",
        "  return Wvi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1OOSUB7HhoAL"
      },
      "outputs": [],
      "source": [
        "# Optimization method\n",
        "class Skipgram(Word2Vec):\n",
        "    \"\"\"A subclass to allow more customization of the Word2Vec internals.\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "\n",
        "        self.vocabulary_counts = None\n",
        "\n",
        "        kwargs[\"min_count\"] = kwargs.get(\"min_count\", 0)\n",
        "        kwargs[\"workers\"] = kwargs.get(\"workers\", cpu_count())\n",
        "        kwargs[\"vector_size\"] = 128\n",
        "        kwargs[\"sentences\"] = kwargs.get(\"sentences\", None)\n",
        "        kwargs[\"window\"] = kwargs.get(\"window\", 10)\n",
        "        kwargs[\"sg\"] = 1\n",
        "        kwargs[\"hs\"] = 1\n",
        "\n",
        "        super(Skipgram, self).__init__(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x1xVPIRyYSXT"
      },
      "outputs": [],
      "source": [
        "def DeepWalk(G, w, d, gamma, t):\n",
        "  '''\n",
        "  Inputs:\n",
        "    G: Graph with vertices V and edges E\n",
        "    w: window size\n",
        "    d: embedding size\n",
        "    gamma: walks per vertex\n",
        "    t: walk length\n",
        "  Outputs:\n",
        "    phi: matrix of vertex representations\n",
        "  '''\n",
        "  vertices_in_G = list(G.nodes)\n",
        "  Wvi = [] # Array where each element is a list of arrays containing the walks that start from that vertex.\n",
        "\n",
        "  for i in range(0, gamma):\n",
        "    random.shuffle(vertices_in_G) # This variable receives the name O in the paper\n",
        "    for vi in vertices_in_G:\n",
        "      Wvi.append(Random_Walk(G, vi, t))\n",
        "\n",
        "  print(\"Finish random walk\")\n",
        "  \n",
        "  phi_model = Skipgram(sentences=Wvi, window=w, min_count=0, trim_rule=None)\n",
        "\n",
        "  return phi_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zoiR1T8whoAM"
      },
      "outputs": [],
      "source": [
        "# Subroutine to label a node with MAX-VOTE --> NOT NECESSARY. The original paper uses this as a baseline\n",
        "def Max_Vote(G, node, k, L):\n",
        "  '''\n",
        "  node: node to label --> v in the paper\n",
        "  neighbors: neighbors of the node to label --> N(v) in the paper\n",
        "  k: Number of labels that will be assigned to the vertex\n",
        "  L: total number of possible labels\n",
        "\n",
        "  Output:\n",
        "    k_lab: most frequent k labels\n",
        "  '''\n",
        "  freq_labels = np.zeros(L)\n",
        "  k_lab = np.full(k , -1) # Set to -1 all the chosen labels\n",
        "  neighbors = list(G.neighbors(node))\n",
        "\n",
        "  for n in neighbors:\n",
        "    labels_neighbor = G.nodes[n]['group'] # Is it necessary to check if the neighbor is empty, or it won't have any effect?\n",
        "    print(labels_neighbor)\n",
        "    for l in labels_neighbor:\n",
        "      freq_labels[l] += 1\n",
        "\n",
        "  used_labels = np.count_nonzero(freq_labels)\n",
        "  for i in range(min(k, used_labels)):\n",
        "    k_lab[i] = np.argmax(freq_labels)\n",
        "    freq_labels[k_lab[i]] = -1 # We mark it as used, but at the same time we differentiate this label and the ones that haven't appeared.\n",
        "\n",
        "  if(k > used_labels): # In case we have to assign more classes than the ones of our neighbors, we choose randomly\n",
        "    zero_indices = np.where(freq_labels == 0)[0]\n",
        "    random_zero_indices = np.random.choice(zero_indices, size=(k - used_labels), replace=False)\n",
        "    k_lab[used_labels:k] = random_zero_indices\n",
        "\n",
        "  return k_lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Wq4qJrV6hoAM"
      },
      "outputs": [],
      "source": [
        "# Some methods need to have all the labels inside an array. Depending on the case, that array has to be composed with arrays/sets containing the labels assigned to each node\n",
        "\n",
        "def get_set_labels(G):\n",
        "    '''\n",
        "    Input:\n",
        "        G: Graph\n",
        "    Output:\n",
        "        labels: List of sets containing labels. Each set corresponds to the groups of id = index + 1\n",
        "    '''\n",
        "\n",
        "    labels = []\n",
        "\n",
        "    for n in G.nodes:\n",
        "        l = set(G.nodes[n].get('group_belonging', []))  # Ensure an empty list is converted to an empty set\n",
        "        labels.append(l)\n",
        "\n",
        "    return labels\n",
        "\n",
        "def get_array_labels(G):\n",
        "  '''\n",
        "  Input:\n",
        "    G: Graph\n",
        "  Output:\n",
        "    labels: Array with labels. Each position of the array will correspond to the groups of id = index + 1\n",
        "  '''\n",
        "\n",
        "  labels = []\n",
        "\n",
        "  for n in G.nodes:\n",
        "      l = G.nodes[n].get('group_belonging')\n",
        "      labels.append(l)\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSZXH0-XL7BQ"
      },
      "source": [
        "### Running DeepWalk in the YouTube Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# dataset = CitationFull(root='/tmp/Cora', name='Cora') ## CORA\n",
        "# dataset = Reddit(\"./\") ## REDDIT\n",
        "# dataset = HeterophilousGraphDataset(root=\"./\", name='amazon_ratings') ## AMAZON RATINGS\n",
        "dataset = Planetoid(root='/tmp/PubMed', name='PubMed') ## PUBMED\n",
        "data = dataset[0]\n",
        "G_YT = to_networkx(data, to_undirected=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes: 19717\n",
            "Number of edges: 88648\n"
          ]
        }
      ],
      "source": [
        "# Access node labels\n",
        "labels = data.y\n",
        "\n",
        "# Convert node labels to a dictionary with node indices as keys\n",
        "group_dict = {i: [labels[i].item()] for i in range(len(labels))}\n",
        "\n",
        "# Add group labels to the nodes\n",
        "for user_id, groups in group_dict.items():\n",
        "    nx.set_node_attributes(G_YT, {user_id: groups}, 'group_belonging')\n",
        "\n",
        "print(\"Number of nodes:\", G_YT.number_of_nodes())\n",
        "print(\"Number of edges:\", G_YT.number_of_edges())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Obtention of a .mat file with the edge list\n",
        "\n",
        "# Obtention of edgelist\n",
        "edgelist = nx.generate_edgelist(G_YT, data=False)\n",
        "#nx.write_edgelist(G_YT, \"pubmed.edgelist\", data=False)\n",
        "\n",
        "# Create a label matrix\n",
        "num_nodes = len(G_YT.nodes)\n",
        "num_classes = len(set(map(lambda x: x[0], list(group_dict.values()))))\n",
        "label_matrix = np.zeros((num_nodes, num_classes))\n",
        "\n",
        "for node, labels in group_dict.items():\n",
        "    for label in labels:\n",
        "        # Subtract 1 from label to adjust for zero-based indexing\n",
        "        label_matrix[node - 1, label - 1] = 1\n",
        "\n",
        "Adj = nx.adjacency_matrix(G_YT)\n",
        "label_matrix_sparse = sparse.csr_matrix(label_matrix)\n",
        "sio.savemat('/Users/silviaarellanogarcia/Documents/MSc MACHINE LEARNING/Advanced Machine Learning/Project/embeddings_deepwalk/pubmed2.mat', {'network': Adj, 'group': label_matrix_sparse})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJKQHDrDt0nK",
        "outputId": "a34cb852-93b3-421e-e81e-301b2b7c1c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finish random walk\n"
          ]
        }
      ],
      "source": [
        "# Graph: G_YT\n",
        "phi_model_YT = DeepWalk(G_YT, window_size, embedding_size, walks_per_vertex, walk_length) # Phi represents the learned embedding matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "tnE2PBn2As7q"
      },
      "outputs": [],
      "source": [
        "# Save the model in Word2Vec format\n",
        "phi_model_YT.wv.save_word2vec_format(SAVE_PATH + '/model_pubmed_directed_80_40.embedding')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjgGD4CChoAQ",
        "outputId": "872ae71b-3edd-4b0f-ef61-6ad3d2159931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.2742879   0.16498137  0.21245699 ...  0.13313136  0.07546434\n",
            "  -0.03327994]\n",
            " [-0.38610804  0.1626573   0.14413613 ... -0.21662538 -0.09398244\n",
            "  -0.03844263]\n",
            " [-0.19908889 -0.5658283  -0.38879186 ...  0.37729552 -0.30201608\n",
            "  -0.10404451]\n",
            " ...\n",
            " [ 0.5277188  -0.29266483 -0.3273233  ...  0.37128177 -0.18906634\n",
            "   0.44112396]\n",
            " [-0.12426006 -0.39650354  0.11178714 ... -0.37924275 -0.45998713\n",
            "   0.47173586]\n",
            " [ 0.29561248 -0.8507624  -0.06788737 ... -0.4310972  -0.7873194\n",
            "  -0.09269765]]\n"
          ]
        }
      ],
      "source": [
        "# Get the embedding vectors\n",
        "phi_vectors_YT = phi_model_YT.wv.vectors\n",
        "print(phi_vectors_YT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "H6aQ4_h3hoAR"
      },
      "outputs": [],
      "source": [
        "# Save the vectors in a numpy file\n",
        "np.save(SAVE_PATH + '/vectors_model_pubmed_directed_80_40.npy', phi_vectors_YT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTzxR8nfhoAS"
      },
      "outputs": [],
      "source": [
        "# LOAD PHI VECTORS YT FROM FILE\n",
        "phi_vectors_YT = np.load(SAVE_PATH + '/vectors_model_cora_directed_80_40.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LOAD PHI MODEL\n",
        "phi_model_YT = KeyedVectors.load_word2vec_format(SAVE_PATH + '/model_cora_80_40.embedding')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 4.45148319e-01, -8.30312744e-02, -3.39485615e-01, ...,\n",
              "        -8.87622610e-02,  2.34206527e-01,  1.55090705e-01],\n",
              "       [ 2.82594532e-01,  2.03033134e-01,  1.44470811e-01, ...,\n",
              "        -1.80515721e-01,  2.15088263e-01,  1.64272547e-01],\n",
              "       [ 3.58948499e-01,  1.77176595e-01, -8.48050117e-02, ...,\n",
              "        -4.79790121e-01, -9.95380506e-02,  1.48593456e-01],\n",
              "       ...,\n",
              "       [-3.99145260e-02,  7.22329691e-02, -8.29841495e-02, ...,\n",
              "        -9.61652398e-03,  3.82785161e-04, -7.63317896e-03],\n",
              "       [ 8.13602135e-02, -9.95194495e-01,  2.53794134e-01, ...,\n",
              "        -3.24240237e-01,  2.48191029e-01, -1.90682977e-01],\n",
              "       [ 5.05192317e-02, -1.01612605e-01, -1.49148166e-01, ...,\n",
              "        -3.09588045e-01,  2.86159873e-01, -2.50462711e-01]], dtype=float32)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualize the loaded vectors\n",
        "phi_model_YT.vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N1Zu0ES3hoAU"
      },
      "outputs": [],
      "source": [
        "# To improve the access to the adjacency matrix, we pass the info to a dictionary, where all the nodes have a entry, and the values aare the neighbours\n",
        "def adj_dictionary(x):\n",
        "    G = defaultdict(lambda: set())\n",
        "    cx = x.tocoo() # Return a COOrdinate representation of this matrix\n",
        "    for i,j,v in zip(cx.row, cx.col, cx.data):\n",
        "        G[i].add(j)\n",
        "    return {str(k): [str(x) for x in v] for k, v in G.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pMdrBL1thoAY"
      },
      "outputs": [],
      "source": [
        "class TopKRanker(OneVsRestClassifier):\n",
        "    def predict(self, X, top_k_list):\n",
        "        assert X.shape[0] == len(top_k_list)\n",
        "        probs = np.asarray(super(TopKRanker, self).predict_proba(X))\n",
        "        all_labels = []\n",
        "        for i, k in enumerate(top_k_list):\n",
        "            probs_ = probs[i, :]\n",
        "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
        "            all_labels.append(labels)\n",
        "        return all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qDsvbWDuhoAY"
      },
      "outputs": [],
      "source": [
        "def get_array_str_labels(G):\n",
        "  '''\n",
        "  Input:\n",
        "    G: Graph\n",
        "  Output:\n",
        "    labels: Array with labels. Each position of the array will correspond to the groups of id = index + 1\n",
        "  '''\n",
        "\n",
        "  labels = []\n",
        "\n",
        "  for n in G.nodes:\n",
        "      l = G.nodes[n].get('group_belonging')\n",
        "      labels.append(str(l))\n",
        "  return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RHfxvq7JhoAZ"
      },
      "outputs": [],
      "source": [
        "# Method used when the dataset doesn't have all the nodes labeled and we need to evaluate\n",
        "\n",
        "def filter_phi(phi, labels_set):\n",
        "    '''\n",
        "    Input:\n",
        "      phi: Embedding matrix\n",
        "      labels_array: Array containing the\n",
        "\n",
        "    Output:\n",
        "      phi_new: Phi Embedding matrix containing only the rows of the labeled nodes\n",
        "    '''\n",
        "    label_np = np.array(labels_set) # Convert the set to a NumPy array\n",
        "\n",
        "    indices_containing_labels = [bool(node_set) for node_set in label_np] # List with True for those indices that correspond to nodes with labels, and False for those which don't have any\n",
        "\n",
        "    # Filter nodes with labels\n",
        "    filtered_labels = label_np[indices_containing_labels] # Numpy array containing the labels of those nodes that are labeled\n",
        "\n",
        "    # Get the indices to keep in the original order\n",
        "    indices_to_keep = np.where(indices_containing_labels)[0] # Id's of the indices (id_nodes - 1) that have a label\n",
        "\n",
        "    # Filter the embedding matrix\n",
        "    filtered_phi = phi[indices_to_keep, :]\n",
        "\n",
        "    return filtered_phi, filtered_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jctoXCiDhoAZ"
      },
      "outputs": [],
      "source": [
        "labels_sets = get_set_labels(G_YT)\n",
        "new_phi, new_labels = filter_phi(phi_vectors_YT, labels_sets)\n",
        "features_matrix = np.asarray([phi_model_YT.wv[str(node)] for node in range(len(G_YT))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ATTENTION!\n",
        "Depending on the dataset, there are some lines that need to be commented/discommented. Check the comments in capital letters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjR_ej8KhoAU",
        "outputId": "b895478c-4409-47d9-95c9-67bf040987e5"
      },
      "outputs": [],
      "source": [
        "adj_G_YT = nx.adjacency_matrix(G_YT, nodelist=None, dtype=None, weight='weight') # Adjacency matrix: Square matrix of N x N size used to represent the connections between the edges of a graph.\n",
        "adj_graph = adj_dictionary(adj_G_YT)\n",
        "\n",
        "num_labels = dataset.num_classes\n",
        "\n",
        "#mlb = MultiLabelBinarizer(classes=range(1, num_labels + 1))\n",
        "mlb = MultiLabelBinarizer(classes=range(num_labels)) ## FOR DATASETS THAT START WITH LABEL 0\n",
        "\n",
        "labels_bin = mlb.fit_transform(new_labels)\n",
        "labels_bin = np.array(labels_bin)\n",
        "\n",
        "try:\n",
        "    features_matrix = np.asarray([phi_model_YT.wv[str(node+1)] for node in range(len(G_YT))])\n",
        "except:\n",
        "    features_matrix = np.asarray([phi_model_YT.wv[str(node)] for node in range(len(G_YT))])\n",
        "\n",
        "shuffles = []\n",
        "for x in range(10): # Decide how many shuffles I want to make\n",
        "    shuffles.append(skshuffle(features_matrix, labels_bin)) # The sklearn shuffle shuffles arrays or sparse matrices in a consistent way.\n",
        "\n",
        "\n",
        "all_results = defaultdict(list)\n",
        "\n",
        "training_percents = [0.1, 0.5, 0.9]\n",
        "\n",
        "for train_percent in training_percents:\n",
        "    for shuf in shuffles:\n",
        "        X, y = shuf ## X corresponds to the phi_vectors and y to the labels.\n",
        "\n",
        "        training_size = int(train_percent * len(X))\n",
        "\n",
        "        X_train = X[:training_size, :]\n",
        "        y_train_ = y[:training_size]\n",
        "\n",
        "        y_train = [[] for x in range(len(y_train_))]\n",
        "\n",
        "        y_train_sparse = coo_matrix(y_train_)\n",
        "        cy =  y_train_sparse.tocoo()\n",
        "\n",
        "        for i, j in zip(cy.row, cy.col):\n",
        "            y_train[i].append(j)\n",
        "\n",
        "        assert sum(len(l) for l in y_train) == np.count_nonzero(y_train_)\n",
        "\n",
        "        X_test = X[training_size:, :]\n",
        "        y_test_ = y[training_size:]\n",
        "\n",
        "        y_test = [[] for _ in range(len(y_test_))]\n",
        "\n",
        "        y_test_sparse = coo_matrix(y_test_)\n",
        "        cy =  y_test_sparse.tocoo()\n",
        "        for i, j in zip(cy.row, cy.col):\n",
        "            y_test[i].append(j)\n",
        "\n",
        "        #print(y_test)\n",
        "        clf = TopKRanker(LogisticRegression()) # creates an instance of TopKRanker with a LogisticRegression model as the base classifier.\n",
        "        clf.fit(X_train, y_train_)\n",
        "\n",
        "        # find out how many labels should be predicted\n",
        "        top_k_list = [len(l) for l in y_test]\n",
        "        preds = clf.predict(X_test, top_k_list)\n",
        "\n",
        "        results = {}\n",
        "        averages = [\"micro\", \"macro\"]\n",
        "        for average in averages:\n",
        "            results[average] = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=average)\n",
        "\n",
        "        all_results[train_percent].append(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP-HFNxFhoAX",
        "outputId": "d64312d5-d754-46c9-c780-4c08cd8b9d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results, using embeddings of dimensionality 128\n",
            "-------------------\n",
            "Train percent: 0.1\n",
            "Train percent: 0.5\n",
            "Train percent: 0.9\n",
            "Shuffle #1:    {'micro': 0.8078093306288032, 'macro': 0.7978228157109367}\n",
            "Shuffle #2:    {'micro': 0.8002028397565923, 'macro': 0.786765798692207}\n",
            "Shuffle #3:    {'micro': 0.8199797160243407, 'macro': 0.8017392153172597}\n",
            "Shuffle #4:    {'micro': 0.8017241379310345, 'macro': 0.7828446256535169}\n",
            "Shuffle #5:    {'micro': 0.8022312373225152, 'macro': 0.7874813351806175}\n",
            "Shuffle #6:    {'micro': 0.8057809330628803, 'macro': 0.7926314412931766}\n",
            "Shuffle #7:    {'micro': 0.8179513184584178, 'macro': 0.8093564877769525}\n",
            "Shuffle #8:    {'micro': 0.8042596348884381, 'macro': 0.7900774490273822}\n",
            "Shuffle #9:    {'micro': 0.813894523326572, 'macro': 0.8034704351492418}\n",
            "Shuffle #10:    {'micro': 0.8113590263691683, 'macro': 0.8026878410592477}\n",
            "Average score: {'micro': 0.8085192697768763, 'macro': 0.7954877444860539}\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "print ('Results, using embeddings of dimensionality', X.shape[1])\n",
        "print ('-------------------')\n",
        "for train_percent in sorted(all_results.keys()):\n",
        "    print ('Train percent:', train_percent)\n",
        "for index, result in enumerate(all_results[train_percent]):\n",
        "    print ('Shuffle #%d:   ' % (index + 1), result)\n",
        "avg_score = defaultdict(float)\n",
        "for score_dict in all_results[train_percent]:\n",
        "    for metric, score in score_dict.items():\n",
        "        avg_score[metric] += score\n",
        "for metric in avg_score:\n",
        "    avg_score[metric] /= len(all_results[train_percent])\n",
        "print ('Average score:', dict(avg_score))\n",
        "print ('-------------------')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
